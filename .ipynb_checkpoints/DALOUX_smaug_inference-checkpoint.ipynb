{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/percevalw/nlstruct.git\n",
    "# pip install git+https://github.com/percevalw/logic_crf.git\n",
    "# pip install z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoann/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yoann/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yoann/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yoann/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yoann/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yoann/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHS (don't forget to change them if need be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the .txt files are stored\n",
    "data_path = \"test_neg\"\n",
    "\n",
    "vocs_path = \"vocs.pkl\" \n",
    "\n",
    "hp_path = \"hp.yaml\"\n",
    "\n",
    "model_path = \"checkpoint-145.pt\"\n",
    "\n",
    "prediction_path = 'preds_test_neg/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dataset(\n",
      "  (docs): 1 * ('doc_id', 'text', 'split')\n",
      ")\n",
      "Transform texts... done\n",
      "Splitting into sentences... Tokenizing... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoann/anaconda3/envs/test_neg/lib/python3.6/site-packages/nlstruct/text/chunking/regex.py:64: FutureWarning: split() requires a non-empty pattern match.\n",
      "  for i, part in enumerate(reg_split.split(txt)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Computing vocabularies...\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized text, with given vocabulary and no unk\n",
      "Normalized text, with given vocabulary and no unk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(vocs_path, \"rb\") as f:\n",
    "    vocs = pkl.load(f)\n",
    "\n",
    "bert_name = \"camembert-base\"\n",
    "dataset = load_from_brat(data_path)\n",
    "\n",
    "docs, sentences, tokens, deltas, _ = preprocess(\n",
    "    dataset=dataset,\n",
    "    max_sentence_length=120,\n",
    "    bert_name=bert_name,\n",
    "    ner_labels= list(vocs['ner_label']),\n",
    "    unknown_labels=\"drop\",\n",
    "    vocabularies=vocs,\n",
    ")\n",
    "\n",
    "prep = Dataset(\n",
    "    sentences=sentences,\n",
    "    tokens=tokens,\n",
    "    deltas=deltas,\n",
    ")\n",
    "\n",
    "batcher, encoded, ids = make_batcher(docs, sentences, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(hp_path, 'r') as f:\n",
    "    hyperparameters = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "scheme = hyperparameters[\"scheme\"]\n",
    "hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "seed = hyperparameters[\"seed\"]\n",
    "lr = hyperparameters[\"lr\"]\n",
    "bert_lr = hyperparameters[\"bert_lr\"]\n",
    "tag_dim = hyperparameters[\"tag_dim\"]\n",
    "token_dim = hyperparameters[\"token_dim\"]\n",
    "max_grad_norm = hyperparameters[\"max_grad_norm\"]\n",
    "tags_lr = hyperparameters[\"tags_lr\"]\n",
    "bert_weight_decay = hyperparameters[\"bert_weight_decay\"]\n",
    "random_perm = hyperparameters[\"random_perm\"]\n",
    "observed_zone_sizes = hyperparameters[\"observed_zone_sizes\"]\n",
    "n_per_zone = hyperparameters[\"n_per_zone\"]\n",
    "n_freeze = hyperparameters[\"n_freeze\"]\n",
    "custom_embeds_layer_index = hyperparameters[\"custom_embeds_layer_index\"]\n",
    "bert_dropout = hyperparameters[\"bert_dropout\"]\n",
    "top_dropout = hyperparameters[\"top_dropout\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load specified model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices 0\n",
      "Current device cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62750d95c074fe38fa6e585b5cee510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=445032417.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "from custom_bert import CustomBertModel\n",
    "from transformers import AdamW, BertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from logic_crf import CRF, ConstraintFactor, HintFactor, Indexer\n",
    "\n",
    "from nlstruct.environment import get_cache, load\n",
    "from nlstruct.utils import evaluating, torch_global as tg, freeze\n",
    "from nlstruct.scoring import compute_metrics, merge_pred_and_gold\n",
    "from nlstruct.train import make_optimizer_and_schedules, run_optimization, seed_all\n",
    "from nlstruct.train.schedule import ScaleOnPlateauSchedule, LinearSchedule, ConstantSchedule\n",
    "    \n",
    "device = torch.device('cpu')\n",
    "tg.set_device(device)\n",
    "\n",
    "# To release gpu memory before allocating new parameters for a new model\n",
    "# A better idea would be to run xp in a function, so that all variables are released when exiting the fn\n",
    "# but this way we can debug after this cell if something goes wrong\n",
    "if \"all_nets\" in globals(): del all_nets\n",
    "if \"state\" in globals(): del state\n",
    "    \n",
    "seed_all(seed) # /!\\ Super important to enable reproducibility\n",
    "\n",
    "ner_net = NERNet(\n",
    "        n_tokens=len(vocs[\"token\"]),\n",
    "        token_dim=token_dim,\n",
    "        n_labels=len(vocs[\"ner_label\"]),\n",
    "        embeddings=CustomBertModel.from_pretrained(bert_name, custom_embeds_layer_index=custom_embeds_layer_index),\n",
    "\n",
    "        dropout=top_dropout,\n",
    "        hidden_dim=hidden_dim,\n",
    "        tag_scheme=scheme,\n",
    "        metric='linear') # cosine might be better but looks less stable, oddly,\n",
    "all_nets = torch.nn.ModuleDict({\n",
    "    \"ner_net\": ner_net,\n",
    "    \"tag_embeddings\": torch.nn.Embedding(ner_net.crf.num_tags - 1, tag_dim),\n",
    "}).to(device=tg.device)\n",
    "del ner_net\n",
    "\n",
    "state = {\"all_nets\": all_nets}  \n",
    "\n",
    "try:\n",
    "    print(f\"Loading from {model_path} ...\")\n",
    "    dumped = torch.load(model_path, map_location=\"cuda:0\")\n",
    "    if dumped is not None:\n",
    "        for name in dumped.keys():\n",
    "            persistable = state.get(name, None)\n",
    "            if name in state and hasattr(persistable, 'load_state_dict'):\n",
    "                persistable.load_state_dict(dumped[name])\n",
    "            else:\n",
    "                state[name] = dumped[name]\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    # We catch any exception otherwise some variables (including torch parameters on the gpu) end up being stored globally in sys.last_value, leading to memory errors)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the inferred mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can reduce batch_size if the model doesn't fit in memory (restart the notebook kernel first to clean memory)\n",
    "pred_batcher = extract_mentions(batcher, all_nets=all_nets, hyperparameters=hyperparameters, batch_size=32)\n",
    "\n",
    "post_mentions = postprocess_batcher(pred_batcher, dataset, prep, ids, vocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to brat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports the predictions to the specified prediction_path \n",
    "# !!! OVERWRITES PREVIOUS PREDICTIONS IN THE PATH !!!\n",
    "preds_to_ann(post_mentions, dataset, vocs, prediction_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_neg",
   "language": "python",
   "name": "test_neg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
